{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gigachat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:running ['2b', '3b', '4b'] on GigaChat-Max\n",
      "2b\n",
      "3b\n",
      "4b\n",
      "  5%|██                                      | 105/2076 [02:23<42:06,  1.28s/it]INFO:openai._base_client:Retrying request to /chat/completions in 0.474491 seconds\n",
      "  5%|█▉                                   | 108/2076 [12:32<49:29:15, 90.53s/it]INFO:openai._base_client:Retrying request to /chat/completions in 0.454344 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.921535 seconds\n",
      "  6%|██▏                                   | 118/2076 [12:55<2:13:57,  4.10s/it]INFO:openai._base_client:Retrying request to /chat/completions in 0.410213 seconds\n",
      "  8%|███▎                                    | 172/2076 [14:23<55:45,  1.76s/it]INFO:openai._base_client:Retrying request to /chat/completions in 0.437965 seconds\n",
      "  9%|███▋                                    | 194/2076 [14:59<45:22,  1.45s/it]INFO:openai._base_client:Retrying request to /chat/completions in 0.479122 seconds\n",
      " 14%|█████▌                                  | 291/2076 [27:30<43:33,  1.46s/it]INFO:openai._base_client:Retrying request to /chat/completions in 0.390764 seconds\n",
      " 32%|████████████▋                           | 659/2076 [47:50<46:56,  1.99s/it]INFO:openai._base_client:Retrying request to /chat/completions in 0.475747 seconds\n",
      " 32%|████████████                          | 661/2076 [47:58<1:07:43,  2.87s/it]INFO:openai._base_client:Retrying request to /chat/completions in 0.472598 seconds\n",
      " 97%|███████████████████████████████████▊ | 2008/2076 [1:20:48<01:28,  1.31s/it]INFO:openai._base_client:Retrying request to /chat/completions in 0.432639 seconds\n",
      "100%|█████████████████████████████████████| 2076/2076 [1:22:35<00:00,  2.39s/it]\n",
      "INFO:root:understanding_rate=0.002890173410404624\n"
     ]
    }
   ],
   "source": [
    "!uv run main.py --model GigaChat-Max --seed 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## haiku-3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:model='claude-3-5-haiku-20241022', encodings=None, understanding_data='data/understanding_test.csv', max_tokens=200, max_params=10000, log_dir='logs', seed=None\n",
      "INFO:root:running ['2b', '3b', '4b'] on claude-3-5-haiku-20241022\n",
      "2b\n",
      "3b\n",
      "4b\n",
      "Batch submitted. Batch ID: msgbatch_01S1NYwEY9XAuhSs9TCRbfz3\n",
      "INFO:root:in_progress\n",
      "INFO:root:in_progress\n",
      "INFO:root:in_progress\n",
      "INFO:root:in_progress\n",
      "INFO:root:in_progress\n",
      "INFO:root:in_progress\n",
      "INFO:root:in_progress\n",
      "INFO:root:in_progress\n",
      "INFO:root:in_progress\n",
      "INFO:root:in_progress\n",
      "INFO:root:in_progress\n",
      "INFO:root:in_progress\n",
      "INFO:root:in_progress\n",
      "INFO:root:in_progress\n",
      "INFO:root:in_progress\n",
      "INFO:root:in_progress\n",
      "INFO:root:in_progress\n",
      "INFO:root:in_progress\n",
      "INFO:root:ended\n",
      "INFO:root:understanding_rate=0.031874630396215256\n"
     ]
    }
   ],
   "source": [
    "!uv run main.py --model claude-3-5-haiku-20241022 --max-tokens 200 --max-params 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## llama-3.3-70B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:model='meta-llama/Llama-3.3-70B-Instruct-Turbo-Free', encodings=None, understanding_data='data/understanding_test.csv', max_tokens=1000, max_params=2500, log_dir='logs', seed=None\n",
      "INFO:root:running ['2b', '3b', '4b'] on meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\n",
      "2b\n",
      "3b\n",
      "4b\n",
      "0\n",
      "100%|█████████████████████████████████████| 5526/5526 [9:20:03<00:00,  6.08s/it]\n",
      "INFO:root:understanding_rate=0.006876583423814694\n"
     ]
    }
   ],
   "source": [
    "!uv run main.py --model meta-llama/Llama-3.3-70B-Instruct-Turbo-Free --max-tokens 1000 --max-params 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:model='deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free', encodings=None, understanding_data='data/understanding_test.csv', max_tokens=2000, max_params=1000, log_dir='logs', seed=None\n",
      "INFO:root:running ['2b', '3b', '4b'] on deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free\n",
      "2b\n",
      "3b\n",
      "4b\n",
      "0\n",
      "  0%|                                       | 2/2526 [00:58<20:30:27, 29.25s/it]\n",
      "\n",
      "Aborted!\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!uv run main.py --model deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free --max-tokens 2000 --max-params 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:model='gpt-4o-mini', encodings=None, understanding_data='data/understanding_test.csv', max_tokens=200, max_params=10000, log_dir='logs', seed=None\n",
      "INFO:root:running ['2b', '3b', '4b'] on gpt-4o-mini\n",
      "2b\n",
      "3b\n",
      "4b\n",
      "Batch submitted. Batch ID: batch_67b989ba328c8190a6b38c9b309d30db\n",
      "INFO:root:batch.status='validating', batch.request_counts=BatchRequestCounts(completed=0, failed=0, total=0)\n",
      "INFO:root:batch.status='in_progress', batch.request_counts=BatchRequestCounts(completed=0, failed=0, total=16910)\n",
      "INFO:root:batch.status='in_progress', batch.request_counts=BatchRequestCounts(completed=265, failed=0, total=16910)\n",
      "INFO:root:batch.status='in_progress', batch.request_counts=BatchRequestCounts(completed=265, failed=0, total=16910)\n",
      "INFO:root:batch.status='in_progress', batch.request_counts=BatchRequestCounts(completed=1075, failed=0, total=16910)\n",
      "INFO:root:batch.status='in_progress', batch.request_counts=BatchRequestCounts(completed=1075, failed=0, total=16910)\n",
      "INFO:root:batch.status='in_progress', batch.request_counts=BatchRequestCounts(completed=1075, failed=0, total=16910)\n",
      "INFO:root:batch.status='in_progress', batch.request_counts=BatchRequestCounts(completed=1075, failed=0, total=16910)\n",
      "INFO:root:batch.status='in_progress', batch.request_counts=BatchRequestCounts(completed=1075, failed=0, total=16910)\n",
      "INFO:root:batch.status='in_progress', batch.request_counts=BatchRequestCounts(completed=1075, failed=0, total=16910)\n",
      "INFO:root:batch.status='in_progress', batch.request_counts=BatchRequestCounts(completed=1075, failed=0, total=16910)\n",
      "INFO:root:batch.status='in_progress', batch.request_counts=BatchRequestCounts(completed=1075, failed=0, total=16910)\n",
      "INFO:root:batch.status='in_progress', batch.request_counts=BatchRequestCounts(completed=1075, failed=0, total=16910)\n",
      "INFO:root:batch.status='in_progress', batch.request_counts=BatchRequestCounts(completed=1075, failed=0, total=16910)\n",
      "INFO:root:batch.status='in_progress', batch.request_counts=BatchRequestCounts(completed=4667, failed=0, total=16910)\n",
      "INFO:root:batch.status='in_progress', batch.request_counts=BatchRequestCounts(completed=4667, failed=0, total=16910)\n",
      "INFO:root:batch.status='in_progress', batch.request_counts=BatchRequestCounts(completed=4667, failed=0, total=16910)\n",
      "INFO:root:batch.status='in_progress', batch.request_counts=BatchRequestCounts(completed=4667, failed=0, total=16910)\n",
      "INFO:root:batch.status='in_progress', batch.request_counts=BatchRequestCounts(completed=4667, failed=0, total=16910)\n",
      "INFO:root:batch.status='in_progress', batch.request_counts=BatchRequestCounts(completed=4667, failed=0, total=16910)\n",
      "INFO:root:batch.status='in_progress', batch.request_counts=BatchRequestCounts(completed=4667, failed=0, total=16910)\n",
      "INFO:root:batch.status='in_progress', batch.request_counts=BatchRequestCounts(completed=4667, failed=0, total=16910)\n",
      "INFO:root:batch.status='in_progress', batch.request_counts=BatchRequestCounts(completed=4667, failed=0, total=16910)\n",
      "INFO:root:batch.status='in_progress', batch.request_counts=BatchRequestCounts(completed=4667, failed=0, total=16910)\n",
      "INFO:root:batch.status='in_progress', batch.request_counts=BatchRequestCounts(completed=4667, failed=0, total=16910)\n",
      "INFO:root:batch.status='in_progress', batch.request_counts=BatchRequestCounts(completed=4667, failed=0, total=16910)\n",
      "INFO:root:batch.status='in_progress', batch.request_counts=BatchRequestCounts(completed=4667, failed=0, total=16910)\n",
      "INFO:root:batch.status='in_progress', batch.request_counts=BatchRequestCounts(completed=4667, failed=0, total=16910)\n",
      "INFO:root:batch.status='in_progress', batch.request_counts=BatchRequestCounts(completed=4667, failed=0, total=16910)\n",
      "INFO:root:batch.status='in_progress', batch.request_counts=BatchRequestCounts(completed=4667, failed=0, total=16910)\n",
      "INFO:root:batch.status='in_progress', batch.request_counts=BatchRequestCounts(completed=4667, failed=0, total=16910)\n",
      "INFO:root:batch.status='in_progress', batch.request_counts=BatchRequestCounts(completed=12737, failed=0, total=16910)\n",
      "INFO:root:batch.status='in_progress', batch.request_counts=BatchRequestCounts(completed=12737, failed=0, total=16910)\n",
      "INFO:root:batch.status='in_progress', batch.request_counts=BatchRequestCounts(completed=12737, failed=0, total=16910)\n",
      "INFO:root:batch.status='in_progress', batch.request_counts=BatchRequestCounts(completed=12737, failed=0, total=16910)\n",
      "INFO:root:batch.status='in_progress', batch.request_counts=BatchRequestCounts(completed=12737, failed=0, total=16910)\n",
      "INFO:root:batch.status='in_progress', batch.request_counts=BatchRequestCounts(completed=12737, failed=0, total=16910)\n",
      "INFO:root:batch.status='in_progress', batch.request_counts=BatchRequestCounts(completed=12737, failed=0, total=16910)\n",
      "INFO:root:batch.status='in_progress', batch.request_counts=BatchRequestCounts(completed=12737, failed=0, total=16910)\n",
      "INFO:root:batch.status='in_progress', batch.request_counts=BatchRequestCounts(completed=12737, failed=0, total=16910)\n",
      "INFO:root:batch.status='in_progress', batch.request_counts=BatchRequestCounts(completed=12737, failed=0, total=16910)\n",
      "INFO:root:batch.status='in_progress', batch.request_counts=BatchRequestCounts(completed=12737, failed=0, total=16910)\n",
      "INFO:root:batch.status='in_progress', batch.request_counts=BatchRequestCounts(completed=12737, failed=0, total=16910)\n",
      "INFO:root:batch.status='in_progress', batch.request_counts=BatchRequestCounts(completed=12737, failed=0, total=16910)\n",
      "INFO:root:batch.status='in_progress', batch.request_counts=BatchRequestCounts(completed=12737, failed=0, total=16910)\n",
      "INFO:root:batch.status='in_progress', batch.request_counts=BatchRequestCounts(completed=12737, failed=0, total=16910)\n",
      "INFO:root:batch.status='in_progress', batch.request_counts=BatchRequestCounts(completed=12737, failed=0, total=16910)\n",
      "INFO:root:batch.status='in_progress', batch.request_counts=BatchRequestCounts(completed=12737, failed=0, total=16910)\n",
      "INFO:root:batch.status='in_progress', batch.request_counts=BatchRequestCounts(completed=16897, failed=0, total=16910)\n",
      "INFO:root:batch.status='in_progress', batch.request_counts=BatchRequestCounts(completed=16897, failed=0, total=16910)\n",
      "INFO:root:batch.status='finalizing', batch.request_counts=BatchRequestCounts(completed=16910, failed=0, total=16910)\n",
      "INFO:root:batch.status='finalizing', batch.request_counts=BatchRequestCounts(completed=16910, failed=0, total=16910)\n",
      "INFO:root:batch.status='finalizing', batch.request_counts=BatchRequestCounts(completed=16910, failed=0, total=16910)\n",
      "INFO:root:batch.status='finalizing', batch.request_counts=BatchRequestCounts(completed=16910, failed=0, total=16910)\n",
      "INFO:root:batch.status='finalizing', batch.request_counts=BatchRequestCounts(completed=16910, failed=0, total=16910)\n",
      "INFO:root:batch.status='finalizing', batch.request_counts=BatchRequestCounts(completed=16910, failed=0, total=16910)\n",
      "INFO:root:batch.status='finalizing', batch.request_counts=BatchRequestCounts(completed=16910, failed=0, total=16910)\n",
      "INFO:root:batch.status='finalizing', batch.request_counts=BatchRequestCounts(completed=16910, failed=0, total=16910)\n",
      "INFO:root:batch.status='finalizing', batch.request_counts=BatchRequestCounts(completed=16910, failed=0, total=16910)\n",
      "INFO:root:batch.status='finalizing', batch.request_counts=BatchRequestCounts(completed=16910, failed=0, total=16910)\n",
      "INFO:root:batch.status='finalizing', batch.request_counts=BatchRequestCounts(completed=16910, failed=0, total=16910)\n",
      "INFO:root:batch.status='finalizing', batch.request_counts=BatchRequestCounts(completed=16910, failed=0, total=16910)\n",
      "INFO:root:batch.status='finalizing', batch.request_counts=BatchRequestCounts(completed=16910, failed=0, total=16910)\n",
      "INFO:root:batch.status='finalizing', batch.request_counts=BatchRequestCounts(completed=16910, failed=0, total=16910)\n",
      "INFO:root:batch.status='finalizing', batch.request_counts=BatchRequestCounts(completed=16910, failed=0, total=16910)\n",
      "INFO:root:batch.status='finalizing', batch.request_counts=BatchRequestCounts(completed=16910, failed=0, total=16910)\n",
      "INFO:root:batch.status='finalizing', batch.request_counts=BatchRequestCounts(completed=16910, failed=0, total=16910)\n",
      "INFO:root:batch.status='finalizing', batch.request_counts=BatchRequestCounts(completed=16910, failed=0, total=16910)\n",
      "INFO:root:batch.status='finalizing', batch.request_counts=BatchRequestCounts(completed=16910, failed=0, total=16910)\n",
      "INFO:root:batch.status='finalizing', batch.request_counts=BatchRequestCounts(completed=16910, failed=0, total=16910)\n",
      "INFO:root:batch.status='finalizing', batch.request_counts=BatchRequestCounts(completed=16910, failed=0, total=16910)\n",
      "INFO:root:batch.status='finalizing', batch.request_counts=BatchRequestCounts(completed=16910, failed=0, total=16910)\n",
      "INFO:root:batch.status='finalizing', batch.request_counts=BatchRequestCounts(completed=16910, failed=0, total=16910)\n",
      "INFO:root:batch.status='finalizing', batch.request_counts=BatchRequestCounts(completed=16910, failed=0, total=16910)\n",
      "INFO:root:batch.status='finalizing', batch.request_counts=BatchRequestCounts(completed=16910, failed=0, total=16910)\n",
      "INFO:root:batch.status='finalizing', batch.request_counts=BatchRequestCounts(completed=16910, failed=0, total=16910)\n",
      "INFO:root:batch.status='finalizing', batch.request_counts=BatchRequestCounts(completed=16910, failed=0, total=16910)\n",
      "INFO:root:batch.status='completed', batch.request_counts=BatchRequestCounts(completed=16910, failed=0, total=16910)\n",
      "INFO:openai._base_client:Retrying request to /files/file-47FgCGL8dB4CG5R1LBqJ4n/content in 0.449822 seconds\n",
      "INFO:root:understanding_rate=0.0010644589000591367\n"
     ]
    }
   ],
   "source": [
    "!uv run main.py --model gpt-4o-mini --max-tokens 200 --max-params 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:model='gpt-4o', encodings=None, understanding_data='data/understanding_test.csv', max_tokens=300, max_params=10000, log_dir='logs', seed=None\n",
      "INFO:root:running ['2b', '3b', '4b'] on gpt-4o\n",
      "2b\n",
      "3b\n",
      "4b\n",
      "Batch submitted. Batch ID: batch_67b99fb7d7848190838d6de0ba4fd646\n",
      "INFO:root:batch.status='validating', batch.request_counts=BatchRequestCounts(completed=0, failed=0, total=0)\n"
     ]
    }
   ],
   "source": [
    "!uv run main.py --model gpt-4o --max-tokens 300 --max-params 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## o3-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
